{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow构建神经网络代码示例",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "phm8UhBKpcBf"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foochane/Tensorflow-neural-network-framework/blob/master/04TensorFlow%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0pJT1MuN3wY",
        "colab_type": "text"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbyczop0g969",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# 导入数据\n",
        "(train_images, train_labels), (test_images, test_labels)= tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUAM8Fe5sZyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据预处理\n",
        "x_train = train_images.reshape((60000, 28 * 28))\n",
        "x_train = x_train.astype('float32') / 255\n",
        "\n",
        "x_test = test_images.reshape((10000, 28 * 28))\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "def num2vocter(y):\n",
        "  v = np.zeros(10)\n",
        "  v[y] =1.0\n",
        "  return v\n",
        "y_train = np.array([num2vocter(y) for y in train_labels])\n",
        "y_test = np.array([num2vocter(y) for y in test_labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgdVMjdOE6s",
        "colab_type": "text"
      },
      "source": [
        "## 版本1：简单神经网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN1yBjffiO36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#定义参数\n",
        "NUM_TRAIN_DATA = len(x_train) #所有训练数据的条数\n",
        "EPOCHS = 50 #训练的轮数\n",
        "BATCH = 1000 #每次喂入神经网络的数据条数\n",
        "\n",
        "#创建网络\n",
        "x = tf.placeholder(tf.float32,[None,784])#定义两个placeholder\n",
        "y = tf.placeholder(tf.float32,[None,10])\n",
        "\n",
        "W = tf.Variable(tf.zeros([784,10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "y_hat = tf.nn.softmax(tf.matmul(x,W)+b)\n",
        "\n",
        "#定义代价函数\n",
        "loss = tf.reduce_mean(tf.square(y-y_hat))\n",
        "\n",
        "#定义优化器\n",
        "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#使用梯度下降法\n",
        "\n",
        "#定义准确率\n",
        "#结果存放在一个布尔型列表中\n",
        "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_hat,1))#argmax返回一维张量中最大的值所在的位置\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "\n",
        "#初始化变量\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#训练模型\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(EPOCHS):  \n",
        "    x_batches = [x_train[k:k+BATCH] for k in range(0,NUM_TRAIN_DATA,BATCH)]\n",
        "    y_batches = [y_train[k:k+BATCH] for k in range(0,NUM_TRAIN_DATA,BATCH)]\n",
        "    for xs,ys in zip(x_batches,y_batches):\n",
        "      sess.run(train_step,feed_dict={x:xs,y:ys})    \n",
        "  \n",
        "    if epoch%10 == 0:\n",
        "      test_accuracy = sess.run(accuracy,feed_dict={x:x_test,y:y_test})\n",
        "      print(\"step %d, test accuracy %g\" %(epoch,test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GadXBpNySfuT",
        "colab_type": "text"
      },
      "source": [
        "## 版本2：多层卷积网络\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWXUI2SXm3wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#每个批次的大小\n",
        "BATCH = 100\n",
        "\n",
        "#训练轮数\n",
        "EPOCHS = 500\n",
        "\n",
        "NUM_TRAIN_DATA = len(x_train) #所有训练数据的条数\n",
        "\n",
        "\n",
        "#构造网络\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "x = tf.placeholder(\"float\", shape=[None, 784])\n",
        "y = tf.placeholder(\"float\", shape=[None, 10])\n",
        "\n",
        "# 第一层卷积\n",
        "# 现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。\n",
        "\n",
        "W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "# 为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。\n",
        "x_image = tf.reshape(x, [-1,28,28,1])\n",
        "# We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. 我们把x_image和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# 第二层卷积\n",
        "# 为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。\n",
        "\n",
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "# 密集连接层\n",
        "# 现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "# Dropout\n",
        "# 为了减少过拟合，我们在输出层之前加入dropout。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。\n",
        "\n",
        "keep_prob = tf.placeholder(\"float\")\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "# 输出层\n",
        "# 最后，我们添加一个softmax层，就像前面的单层softmax regression一样。\n",
        "\n",
        "\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y_hat = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "\n",
        "# 定义代价函数\n",
        "loss = tf.reduce_mean(tf.square(y-y_hat))\n",
        "# cross_entropy = -tf.reduce_sum(y*tf.log(y_hat))\n",
        "\n",
        "# 定义优化器\n",
        "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#使用梯度下降法\n",
        "# train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "#定义准确率\n",
        "correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "# 训练模型\n",
        "with tf.Session() as sess:\n",
        "\n",
        "  #初始化参数\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.local_variables_initializer())\n",
        "\n",
        "  for epoch in range(EPOCHS):  \n",
        "    x_batches = [x_train[k:k+BATCH] for k in range(0,NUM_TRAIN_DATA,BATCH)]\n",
        "    y_batches = [y_train[k:k+BATCH] for k in range(0,NUM_TRAIN_DATA,BATCH)]\n",
        "    for xs,ys in zip(x_batches,y_batches):\n",
        "      sess.run(train_step,feed_dict={x:xs,y:ys,keep_prob: 0.5})   \n",
        "  \n",
        "    if epoch%10 == 0:\n",
        "      train_accuracy = sess.run(accuracy,feed_dict={x:xs, y: ys, keep_prob: 1.0})\n",
        "      test_accuracy = sess.run(accuracy,feed_dict={x:x_test,y:y_test,keep_prob: 1.0})\n",
        "      print(\"step %d, train accuracy %g,test accuracy %g\" %(epoch,train_accuracy,test_accuracy))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmj2MtdkY1KF",
        "colab_type": "text"
      },
      "source": [
        "## 版本3: Tensorflow旧版"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdTZD84aZjbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"MNIST_data\"):\n",
        "  os.mkdir(\"MNIST_data\")\n",
        "\n",
        "!wget -P ./MNIST_data https://github.com/foochane/Tensorflow-neural-network-framework/raw/master/MNIST_data/t10k-images-idx3-ubyte.gz\n",
        "!wget -P ./MNIST_data https://github.com/foochane/Tensorflow-neural-network-framework/raw/master/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
        "!wget -P ./MNIST_data https://github.com/foochane/Tensorflow-neural-network-framework/raw/master/MNIST_data/train-images-idx3-ubyte.gz\n",
        "!wget -P ./MNIST_data https://github.com/foochane/Tensorflow-neural-network-framework/raw/master/MNIST_data/train-labels-idx1-ubyte.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be2JDCXPaFe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "#每个批次的大小\n",
        "BATCH = 100\n",
        "\n",
        "#训练轮数\n",
        "EPOCHS = 500\n",
        "\n",
        "\n",
        "def weight_variable(shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "x = tf.placeholder(\"float\", shape=[None, 784])\n",
        "y = tf.placeholder(\"float\", shape=[None, 10])\n",
        "\n",
        "# 第一层卷积\n",
        "# 现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。\n",
        "\n",
        "W_conv1 = weight_variable([5, 5, 1, 32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "# 为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。\n",
        "x_image = tf.reshape(x, [-1,28,28,1])\n",
        "# We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. 我们把x_image和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "# 第二层卷积\n",
        "# 为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。\n",
        "\n",
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "# 密集连接层\n",
        "# 现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。\n",
        "\n",
        "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "# Dropout\n",
        "# 为了减少过拟合，我们在输出层之前加入dropout。我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。\n",
        "\n",
        "keep_prob = tf.placeholder(\"float\")\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "# 输出层\n",
        "# 最后，我们添加一个softmax层，就像前面的单层softmax regression一样。\n",
        "\n",
        "\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "\n",
        "y_hat = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
        "\n",
        "# 定义代价函数\n",
        "loss = tf.reduce_mean(tf.square(y-y_hat))\n",
        "# cross_entropy = -tf.reduce_sum(y*tf.log(y_hat))\n",
        "\n",
        "# 定义优化器\n",
        "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#使用梯度下降法\n",
        "# train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "#定义准确率\n",
        "correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "\n",
        "#训练模型\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(EPOCHS):\n",
        "    # batch = mnist.train.next_batch(BATCH)\n",
        "    # train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
        "\n",
        "    train_step.run(feed_dict={x: mnist.train.images[:1000], y: mnist.train.labels[:1000], keep_prob: 0.5})\n",
        "\n",
        "    if i%10 == 0:\n",
        "      # train_accuracy = accuracy.eval(feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
        "      train_accuracy = sess.run(accuracy,feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
        "\n",
        "      # test_accuracy = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
        "      test_accuracy = sess.run(accuracy,feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
        "      print(\"step %d, train accuracy %g,test accuracy %g\" %(epoch,train_accuracy,test_accuracy))\n",
        "\n",
        "\n",
        "# eval() 其实就是tf.Tensor的Session.run() 的另外一种写法。加上一个Session context manager：\n",
        "# with tf.Session() as sess:\n",
        "#   print(accuracy.eval({x:mnist.test.images,y_: mnist.test.labels}))\n",
        "  \n",
        "# 其效果和下面的代码是等价的：\n",
        "# with tf.Session() as sess:\n",
        "#   print(sess.run(accuracy, {x:mnist.test.images,y_: mnist.test.labels}))\n",
        "  \n",
        "# 但是要注意的是，eval()只能用于tf.Tensor类对象，也就是有输出的Operation。对于没有输出的Operation, 可以用.run()或者Session.run()。Session.run()没有这个限制。\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phm8UhBKpcBf",
        "colab_type": "text"
      },
      "source": [
        "## 版本4：Keras版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH09sKr4WEUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# 训练集、验证集和测试集, 从训练集种取出10000个样本作为验证集\n",
        "x_train = train_images[:50000]\n",
        "y_train = train_labels[:50000]\n",
        "\n",
        "x_validation = train_images[50000:]\n",
        "y_validation = train_labels[50000:]\n",
        "\n",
        "x_test = test_images\n",
        "y_test = test_labels\n",
        "\n",
        "\n",
        "# 2 定义模型\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 3 配置优化器、损失函数和指标\n",
        "model.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# 4 训练\n",
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=100, \n",
        "                    batch_size=1000,\n",
        "                    validation_data=(x_validation, y_validation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JjxvCuBkUKp",
        "colab_type": "text"
      },
      "source": [
        "## 版本5：Tensorflow新版"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBl3AbY5kUsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}